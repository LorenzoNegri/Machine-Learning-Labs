{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Gradient Descent Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will code a gradient descent module from scratch here to play with internal workings of the optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will have the following parameters:\n",
    " - ***x***: Independent variable\n",
    " - ***y***: Dependent variable\n",
    " - ***learn_rate***: the rate which gradients are updated; low causes slow convergence and high causes divergence and missing the global minimum\n",
    " - ***conv_threshold***: the difference between the old MSE(Mean Square Error) and new MSE on each iteration\n",
    " - ***batch_size***: number of observations considered at each iteration fo updating gradients; high number causes lower iterations, and lower number causes decrease in errors. Ideally the number should be a value of 30 due to statistical significance.\n",
    " - ***max_iter***: maximum number of iteration, beyond which the algorithm will be stoped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"gradient_descent - my module\"\"\"\n",
    "import numpy as np\n",
    "def myDescent(x, y, learn_rate, conv_threshold, batch_size, max_iter):\n",
    "    \"\"\"\n",
    "    myDescent(x, y, learn_rate, conv_threshold, batch_size, max_iter) parameters:\n",
    " - x: Independent variable\n",
    " - y: Dependent variable\n",
    " - learn_rate: the rate which gradients are updated; low causes slow convergence and high causes divergence and missing the global minimum\n",
    " - conv_threshold: the difference between the old MSE(Mean Square Error) and new MSE on each iteration\n",
    " - batch_size: number of observations considered at each iteration fo updating gradients; high number causes lower iterations, and lower number causes decrease in errors. Ideally the number should be a value of 30 due to statistical significance.\n",
    " - max_iter: maximum number of iteration, beyond which the algorithm will be stoped.\n",
    "    \n",
    "    \"\"\"\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = batch_size\n",
    "    t0 = np.random.random(x.shape[1])\n",
    "    t1 = np.random.random(x.shape[1])\n",
    "    MSE = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])/m\n",
    "    while not converged:\n",
    "        grad0 = 1.0/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)])\n",
    "        grad1 = 1.0/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n",
    "        temp0 = t0 - learn_rate * grad0\n",
    "        temp1 = t1 - learn_rate * grad1\n",
    "        t0 = temp0\n",
    "        t1 = temp1\n",
    "        MSE_new = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])/m\n",
    "        if abs(MSE - MSE_new) <= conv_threshold:\n",
    "            print('Converged iterations: ', iter)\n",
    "            converged = True\n",
    "        MSE = MSE_new\n",
    "        iter += 1\n",
    "        if iter == max_iter:\n",
    "            print('Max interactions reached')\n",
    "            converged = True\n",
    "        return t0,t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
